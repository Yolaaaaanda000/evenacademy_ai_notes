# å¼€å‘è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

## ğŸ¬ **è§†é¢‘å¤„ç†ç›¸å…³é—®é¢˜**

### é—®é¢˜1: è§†é¢‘è½¬å½•å‡†ç¡®ç‡ä½
**ç—‡çŠ¶**: Whisperè½¬å½•ç»“æœåŒ…å«å¤§é‡é”™è¯¯ï¼Œå½±å“åç»­åˆ†æ

**åŸå› åˆ†æ**:
- éŸ³é¢‘è´¨é‡å·®ï¼ˆèƒŒæ™¯å™ªéŸ³ã€å›éŸ³ï¼‰
- è¯­é€Ÿè¿‡å¿«æˆ–å£éŸ³é‡
- ä¸“ä¸šæœ¯è¯­è¯†åˆ«ä¸å‡†ç¡®
- å¤šäººå¯¹è¯æ··æ·†

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. éŸ³é¢‘é¢„å¤„ç†
def preprocess_audio(audio_path):
    # é™å™ªå¤„ç†
    audio = AudioSegment.from_file(audio_path)
    audio = audio.normalize()  # æ ‡å‡†åŒ–éŸ³é‡
    audio = audio.low_pass_filter(3000)  # è¿‡æ»¤é«˜é¢‘å™ªéŸ³
    return audio

# 2. å¤šæ¨¡å‹ensemble
def robust_transcription(audio_path):
    models = ["base", "small", "medium"]
    results = []
    for model in models:
        result = transcribe_with_model(audio_path, model)
        results.append(result)
    return combine_results(results)  # æŠ•ç¥¨æœºåˆ¶
```

**è¿›ä¸€æ­¥ä¼˜åŒ–**:
- ä½¿ç”¨æ›´å¤§çš„Whisperæ¨¡å‹ï¼ˆmedium/largeï¼‰
- é’ˆå¯¹æ•™è‚²é¢†åŸŸfine-tuneæ¨¡å‹
- äººå·¥æ ¡å¯¹å…³é”®æœ¯è¯­è¯å…¸

---

### é—®é¢˜2: æ—¶é—´æˆ³åŒ¹é…ä¸å‡†ç¡®
**ç—‡çŠ¶**: ç‚¹å‡»Summaryä¸­çš„æ¦‚å¿µæ— æ³•å‡†ç¡®è·³è½¬åˆ°å¯¹åº”è§†é¢‘ä½ç½®

**åŸå› åˆ†æ**:
- è½¬å½•æ–‡æœ¬ä¸å®é™…è¯­éŸ³ä¸å®Œå…¨å¯¹åº”
- å¥å­è¾¹ç•Œè¯†åˆ«é”™è¯¯
- å¤šä¸ªæ¦‚å¿µåœ¨åŒä¸€æ—¶é—´æ®µå‡ºç°

**è§£å†³æ–¹æ¡ˆ**:
```python
def _find_matching_timestamps(self, start_phrase, end_phrase, key_phrase, transcription):
    # 1. å¤šçº§åŒ¹é…ç­–ç•¥
    # å®Œå…¨åŒ¹é… -> éƒ¨åˆ†åŒ¹é… -> å…³é”®è¯åŒ¹é… -> è¯­ä¹‰åŒ¹é…
    
    # 2. æ¨¡ç³ŠåŒ¹é…ç®—æ³•
    from difflib import SequenceMatcher
    
    def fuzzy_match(target, candidates, threshold=0.6):
        best_match = None
        best_score = 0
        for candidate in candidates:
            score = SequenceMatcher(None, target.lower(), candidate.lower()).ratio()
            if score > threshold and score > best_score:
                best_match = candidate
                best_score = score
        return best_match
    
    # 3. æ—¶é—´æ®µæ‰©å±•
    if end_time <= start_time:
        end_time = min(start_time + 30, total_duration)  # è‡³å°‘30ç§’
```

**ç›‘æ§å’Œè°ƒè¯•**:
- è®°å½•åŒ¹é…æˆåŠŸç‡ç»Ÿè®¡
- å¯è§†åŒ–æ—¶é—´æˆ³åŒ¹é…ç»“æœ
- ç”¨æˆ·åé¦ˆæœºåˆ¶ä¼˜åŒ–ç®—æ³•

---

## ğŸ¤– **AIå¤„ç†ç›¸å…³é—®é¢˜**

### é—®é¢˜3: LLMå“åº”ä¸ç¨³å®š
**ç—‡çŠ¶**: åŒæ ·çš„è¾“å…¥äº§ç”Ÿä¸åŒæ ¼å¼çš„è¾“å‡ºï¼Œè§£æå¤±è´¥

**åŸå› åˆ†æ**:
- LLMè¾“å‡ºçš„éšæœºæ€§
- PromptæŒ‡ä»¤ä¸å¤Ÿæ˜ç¡®
- è¾“å‡ºæ ¼å¼éªŒè¯ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä¸¥æ ¼çš„Promptè®¾è®¡
prompt_template = """
ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°ï¼š

{
  "content_type": "æ¦‚å¿µè®²è§£/é¢˜ç›®å¤ä¹ /æ··åˆå†…å®¹",
  "confidence": 0.95,
  "segments": [
    {
      "title": "ç‰‡æ®µæ ‡é¢˜",
      "start_phrase": "å¼€å§‹å¥å­",
      "end_phrase": "ç»“æŸå¥å­"
    }
  ]
}

é‡è¦ï¼šåªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«```json```æ ‡è®°ã€‚
"""

# 2. è¾“å‡ºéªŒè¯å’Œé‡è¯•æœºåˆ¶
def robust_llm_call(prompt, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            cleaned_response = clean_json_output(response.text)
            result = json.loads(cleaned_response)
            validate_output_format(result)  # éªŒè¯å¿…éœ€å­—æ®µ
            return result
        except (json.JSONDecodeError, ValidationError) as e:
            if attempt == max_retries - 1:
                return fallback_result()
            time.sleep(1)  # çŸ­æš‚ç­‰å¾…åé‡è¯•
```

**Promptå·¥ç¨‹æœ€ä½³å®è·µ**:
- ä½¿ç”¨few-shot examples
- æ˜ç¡®è¾“å‡ºæ ¼å¼è¦æ±‚
- æ·»åŠ æ ¼å¼éªŒè¯è¯´æ˜
- è®¾ç½®temperature=0å‡å°‘éšæœºæ€§

---

### é—®é¢˜4: Summaryè´¨é‡ä¸ä¸€è‡´
**ç—‡çŠ¶**: ç”Ÿæˆçš„Summaryæœ‰æ—¶è¿‡äºç®€å•ï¼Œæœ‰æ—¶è¿‡äºå¤æ‚

**åŸå› åˆ†æ**:
- è§†é¢‘å†…å®¹å¤æ‚åº¦å·®å¼‚å¤§
- Promptæ²¡æœ‰è€ƒè™‘ä¸åŒéš¾åº¦çº§åˆ«
- ç¼ºä¹è´¨é‡è¯„ä¼°æœºåˆ¶

**è§£å†³æ–¹æ¡ˆ**:
```python
class AdaptiveSummaryGenerator:
    def generate_summary(self, analysis, transcription, lecture_title, language):
        # 1. å†…å®¹å¤æ‚åº¦è¯„ä¼°
        complexity = self.assess_content_complexity(analysis)
        
        # 2. åŠ¨æ€Prompté€‰æ‹©
        if complexity == "high":
            prompt_template = self.get_detailed_prompt()
        elif complexity == "low":
            prompt_template = self.get_concise_prompt()
        else:
            prompt_template = self.get_balanced_prompt()
        
        # 3. è´¨é‡æ£€éªŒ
        summary = self.generate_with_template(prompt_template)
        quality_score = self.validate_summary_quality(summary)
        
        if quality_score < 0.7:
            # ä½¿ç”¨å¤‡ç”¨ç­–ç•¥é‡æ–°ç”Ÿæˆ
            summary = self.regenerate_with_fallback(analysis)
        
        return summary
    
    def validate_summary_quality(self, summary):
        checks = {
            'has_course_overview': '## Course Overview' in summary,
            'has_knowledge_points': '## Main Knowledge Points' in summary,
            'min_length': len(summary) > 500,
            'has_structure': summary.count('##') >= 3
        }
        return sum(checks.values()) / len(checks)
```

---

## ğŸ’¾ **æ€§èƒ½å’Œç¼“å­˜é—®é¢˜**

### é—®é¢˜5: é‡å¤å¤„ç†æµªè´¹èµ„æº
**ç—‡çŠ¶**: åŒä¸€è§†é¢‘è¢«é‡å¤è½¬å½•å’Œåˆ†æï¼Œå¤„ç†æ—¶é—´é•¿

**è§£å†³æ–¹æ¡ˆ**: æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
```python
class SmartCacheManager:
    def __init__(self):
        self.cache_dir = "./data/cache"
        self.cache_metadata = self.load_cache_metadata()
    
    def get_cache_key(self, video_path, lecture_title, language):
        # åŸºäºæ–‡ä»¶å†…å®¹hash + å‚æ•°ç”Ÿæˆå”¯ä¸€key
        file_hash = self.get_file_hash(video_path)
        param_hash = hashlib.md5(f"{lecture_title}_{language}".encode()).hexdigest()[:8]
        return f"{param_hash}_{file_hash}"
    
    def is_cache_valid(self, cache_key):
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸã€å®Œæ•´
        cache_info = self.cache_metadata.get(cache_key)
        if not cache_info:
            return False
        
        # æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
        cache_file = cache_info['file_path']
        if not os.path.exists(cache_file):
            return False
        
        # æ£€æŸ¥ç¼“å­˜ç‰ˆæœ¬å…¼å®¹æ€§
        if cache_info.get('version') != self.current_version:
            return False
        
        return True
```

---

### é—®é¢˜6: å†…å­˜æº¢å‡ºå’Œå¤„ç†è¶…æ—¶
**ç—‡çŠ¶**: å¤„ç†å¤§è§†é¢‘æ–‡ä»¶æ—¶ç³»ç»Ÿå´©æºƒæˆ–è¶…æ—¶

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. åˆ†æ®µå¤„ç†
def process_large_video(video_path, max_segment_duration=600):  # 10åˆ†é’Ÿåˆ†æ®µ
    video_duration = get_video_duration(video_path)
    segments = []
    
    for start_time in range(0, int(video_duration), max_segment_duration):
        end_time = min(start_time + max_segment_duration, video_duration)
        segment_path = extract_video_segment(video_path, start_time, end_time)
        segment_result = process_video_segment(segment_path)
        segments.append(segment_result)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(segment_path)
    
    return merge_segment_results(segments)

# 2. å¼‚æ­¥å¤„ç†
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def async_video_processing(video_path, lecture_title, language):
    loop = asyncio.get_event_loop()
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        # å¹¶è¡Œå¤„ç†ä¸åŒä»»åŠ¡
        transcription_task = loop.run_in_executor(executor, transcribe_video, video_path)
        analysis_task = loop.run_in_executor(executor, analyze_content, video_path)
        
        transcription, analysis = await asyncio.gather(transcription_task, analysis_task)
        
        # åˆå¹¶ç»“æœ
        return combine_results(transcription, analysis)
```

---

## ğŸ¨ **å‰ç«¯é›†æˆé—®é¢˜**

### é—®é¢˜7: é¡µé¢æ•°æ®åŒæ­¥æ··ä¹±
**ç—‡çŠ¶**: è§†é¢‘æ’­æ”¾è¿›åº¦ä¸çŸ¥è¯†ç‚¹é«˜äº®ä¸åŒæ­¥ï¼Œæ•°æ®æ›´æ–°å†²çª

**è§£å†³æ–¹æ¡ˆ**: çŠ¶æ€ç®¡ç†å’Œäº‹ä»¶ç³»ç»Ÿ
```javascript
// 1. ç»Ÿä¸€çŠ¶æ€ç®¡ç†
class VideoPlayerState {
    constructor() {
        this.currentTime = 0;
        this.activeKnowledgePoint = null;
        this.timestampMapping = {};
        this.isPlaying = false;
        this.listeners = [];
    }
    
    updateTime(newTime) {
        this.currentTime = newTime;
        this.notifyListeners('timeUpdate', newTime);
        this.updateActiveKnowledgePoint();
    }
    
    updateActiveKnowledgePoint() {
        const newActive = this.findActiveKnowledgePoint(this.currentTime);
        if (newActive !== this.activeKnowledgePoint) {
            this.activeKnowledgePoint = newActive;
            this.notifyListeners('knowledgePointChange', newActive);
        }
    }
    
    notifyListeners(event, data) {
        this.listeners.forEach(listener => {
            if (listener.event === event) {
                listener.callback(data);
            }
        });
    }
}

// 2. é˜²æŠ–å’ŒèŠ‚æµ
function debounce(func, wait) {
    let timeout;
    return function executedFunction(...args) {
        const later = () => {
            clearTimeout(timeout);
            func(...args);
        };
        clearTimeout(timeout);
        timeout = setTimeout(later, wait);
    };
}

const debouncedTimeUpdate = debounce(updateTimeDisplay, 100);
```

---

### é—®é¢˜8: ç”¨æˆ·ä½“éªŒä¸è¿è´¯
**ç—‡çŠ¶**: é¡µé¢è·³è½¬å¯¼è‡´çŠ¶æ€ä¸¢å¤±ï¼ŒåŠ è½½æ—¶é—´é•¿ï¼Œæ“ä½œä¸ç›´è§‚

**è§£å†³æ–¹æ¡ˆ**: å•é¡µåº”ç”¨è®¾è®¡
```javascript
// 1. æ¸è¿›å¼åŠ è½½
class ProgressiveLoader {
    async loadVideoData(videoFile) {
        // ç«‹å³æ˜¾ç¤ºè§†é¢‘é¢„è§ˆ
        this.showVideoPreview(videoFile);
        
        // åˆ†æ­¥éª¤æ˜¾ç¤ºå¤„ç†è¿›åº¦
        this.showProcessingSteps();
        
        // åå°å¤„ç†ï¼Œå®æ—¶æ›´æ–°è¿›åº¦
        const result = await this.processWithProgress(videoFile);
        
        // é€æ­¥æ˜¾ç¤ºç»“æœ
        this.showKnowledgePoints(result.knowledgePoints);
        this.showSummary(result.summary);
        this.enableInteractions();
    }
    
    showProcessingSteps() {
        const steps = ['è½¬å½•éŸ³é¢‘', 'åˆ†æå†…å®¹', 'åŒ¹é…æ—¶é—´æˆ³', 'ç”ŸæˆSummary'];
        steps.forEach((step, index) => {
            setTimeout(() => {
                this.activateStep(index);
            }, index * 1000);
        });
    }
}

// 2. ä¼˜é›…é™çº§
function handleError(error) {
    console.error('å¤„ç†å‡ºé”™:', error);
    
    // æ˜¾ç¤ºå‹å¥½çš„é”™è¯¯ä¿¡æ¯
    showUserFriendlyError(error.type);
    
    // æä¾›å¤‡é€‰æ–¹æ¡ˆ
    if (error.type === 'transcription_failed') {
        offerManualTranscription();
    } else if (error.type === 'analysis_failed') {
        showBasicVideoPlayer();
    }
}
```

---

## ğŸ”§ **ç³»ç»Ÿæ¶æ„é—®é¢˜**

### é—®é¢˜9: ä»£ç è€¦åˆåº¦é«˜ï¼Œéš¾ä»¥ç»´æŠ¤
**ç—‡çŠ¶**: ä¿®æ”¹ä¸€ä¸ªåŠŸèƒ½å½±å“å…¶ä»–æ¨¡å—ï¼Œæµ‹è¯•å›°éš¾

**è§£å†³æ–¹æ¡ˆ**: æ¨¡å—åŒ–æ¶æ„
```python
# 1. ä¾èµ–æ³¨å…¥
class VideoProcessor:
    def __init__(self, transcriber, analyzer, summarizer, cache_manager):
        self.transcriber = transcriber
        self.analyzer = analyzer
        self.summarizer = summarizer
        self.cache_manager = cache_manager
    
    def process_video(self, video_path, options):
        # æ¯ä¸ªç»„ä»¶èŒè´£å•ä¸€ï¼Œå¯ç‹¬ç«‹æµ‹è¯•
        cache_key = self.cache_manager.get_cache_key(video_path, options)
        
        if self.cache_manager.has_valid_cache(cache_key):
            return self.cache_manager.load_cache(cache_key)
        
        transcription = self.transcriber.transcribe(video_path)
        analysis = self.analyzer.analyze(transcription)
        summary = self.summarizer.generate(analysis)
        
        result = self.combine_results(transcription, analysis, summary)
        self.cache_manager.save_cache(cache_key, result)
        
        return result

# 2. æ¥å£å®šä¹‰
from abc import ABC, abstractmethod

class TranscriberInterface(ABC):
    @abstractmethod
    def transcribe(self, audio_path: str) -> Dict:
        pass

class WhisperTranscriber(TranscriberInterface):
    def transcribe(self, audio_path: str) -> Dict:
        # Whisperå®ç°
        pass

class GoogleSpeechTranscriber(TranscriberInterface):
    def transcribe(self, audio_path: str) -> Dict:
        # Google Speechå®ç°
        pass
```

---

### é—®é¢˜10: é…ç½®ç®¡ç†æ··ä¹±
**ç—‡çŠ¶**: å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒé…ç½®å†²çªï¼ŒAPIå¯†é’¥æ³„éœ²

**è§£å†³æ–¹æ¡ˆ**: é…ç½®ç®¡ç†ç³»ç»Ÿ
```python
# config.py
class Config:
    def __init__(self):
        self.load_from_env()
        self.validate_config()
    
    def load_from_env(self):
        self.GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
        self.CACHE_DIR = os.getenv('CACHE_DIR', './data/cache')
        self.MAX_VIDEO_SIZE = int(os.getenv('MAX_VIDEO_SIZE', '500MB'))
        self.DEBUG = os.getenv('DEBUG', 'False').lower() == 'true'
    
    def validate_config(self):
        required_vars = ['GEMINI_API_KEY']
        missing = [var for var in required_vars if not getattr(self, var)]
        if missing:
            raise EnvironmentError(f"Missing required environment variables: {missing}")

# .envæ–‡ä»¶
GEMINI_API_KEY=your_api_key_here
CACHE_DIR=./data/cache
MAX_VIDEO_SIZE=500MB
DEBUG=false
```

---

## ğŸ“Š **ç›‘æ§å’Œè°ƒè¯•é—®é¢˜**

### é—®é¢˜11: éš¾ä»¥è¿½è¸ªå¤„ç†è¿‡ç¨‹å’Œæ€§èƒ½ç“¶é¢ˆ
**è§£å†³æ–¹æ¡ˆ**: æ—¥å¿—å’Œç›‘æ§ç³»ç»Ÿ
```python
import logging
import time
from functools import wraps

# 1. ç»“æ„åŒ–æ—¥å¿—
class StructuredLogger:
    def __init__(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('video_processing.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_processing_step(self, step_name, video_title, duration=None, success=True):
        log_data = {
            'step': step_name,
            'video_title': video_title,
            'duration': duration,
            'success': success,
            'timestamp': time.time()
        }
        self.logger.info(f"Processing step: {json.dumps(log_data)}")

# 2. æ€§èƒ½ç›‘æ§è£…é¥°å™¨
def monitor_performance(step_name):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                logger.log_processing_step(step_name, success=True, duration=duration)
                return result
            except Exception as e:
                duration = time.time() - start_time
                logger.log_processing_step(step_name, success=False, duration=duration)
                raise
        return wrapper
    return decorator

@monitor_performance("video_transcription")
def transcribe_video(video_path):
    # è½¬å½•é€»è¾‘
    pass
```

---

## ğŸ¯ **é—®é¢˜è§£å†³çš„ç»éªŒæ€»ç»“**

### **é¢„é˜²æ€§æªæ–½**:
1. **å…¨é¢æµ‹è¯•**: å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯• + ç«¯åˆ°ç«¯æµ‹è¯•
2. **é”™è¯¯å¤„ç†**: æ¯ä¸ªå…³é”®æ“ä½œéƒ½æœ‰fallbackæ–¹æ¡ˆ
3. **ç›‘æ§æŠ¥è­¦**: å…³é”®æŒ‡æ ‡å¼‚å¸¸æ—¶åŠæ—¶é€šçŸ¥
4. **æ–‡æ¡£å®Œå–„**: ä»£ç æ³¨é‡Š + APIæ–‡æ¡£ + ç”¨æˆ·æ‰‹å†Œ

### **è°ƒè¯•æŠ€å·§**:
1. **åˆ†å±‚è°ƒè¯•**: ä»å‰ç«¯åˆ°åç«¯é€å±‚æ’æŸ¥
2. **æ•°æ®éªŒè¯**: å…³é”®èŠ‚ç‚¹æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§
3. **æ€§èƒ½åˆ†æ**: ä½¿ç”¨profileræ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆ
4. **ç”¨æˆ·åé¦ˆ**: æ”¶é›†çœŸå®ä½¿ç”¨åœºæ™¯çš„é—®é¢˜

### **è¿­ä»£ä¼˜åŒ–**:
1. **MVPå…ˆè¡Œ**: å…ˆå®ç°æ ¸å¿ƒåŠŸèƒ½ï¼Œå†é€æ­¥å®Œå–„
2. **ç”¨æˆ·é©±åŠ¨**: æ ¹æ®å®é™…ä½¿ç”¨åé¦ˆè°ƒæ•´ä¼˜å…ˆçº§
3. **æŠ€æœ¯å€ºåŠ¡ç®¡ç†**: å®šæœŸé‡æ„ï¼Œä¿æŒä»£ç è´¨é‡
4. **æŒç»­å­¦ä¹ **: è·Ÿè¿›æ–°æŠ€æœ¯ï¼Œä¼˜åŒ–ç°æœ‰æ–¹æ¡ˆ

é€šè¿‡è¿™ç§ç³»ç»Ÿæ€§çš„é—®é¢˜è¯†åˆ«å’Œè§£å†³æ–¹æ³•ï¼Œæ‚¨çš„é¡¹ç›®èƒ½å¤Ÿæ›´ç¨³å®šã€é«˜æ•ˆåœ°ä¸ºç”¨æˆ·æä¾›æœåŠ¡ã€‚