"""
Vercel-optimized VideoProcessor with improved cache handling
é’ˆå¯¹Verceléƒ¨ç½²ä¼˜åŒ–çš„è§†é¢‘å¤„ç†å™¨ï¼Œæ”¹è¿›ç¼“å­˜å¤„ç†é€»è¾‘
"""

import hashlib
import os
import tempfile
import json
from datetime import datetime
import re
from typing import List, Dict, Tuple, Optional
from core.summary_integrator import SummaryIntegrator
from core.prompt_manager import PromptManager


class VideoProcessor:
    def __init__(self, api_key: str, cache_only_mode=False):
        """åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨ - Vercelä¼˜åŒ–ç‰ˆæœ¬"""
        self.cache_only_mode = cache_only_mode
        
        # Vercelç¯å¢ƒæ£€æµ‹
        self.is_vercel = os.environ.get('VERCEL') == '1'
        
        # é…ç½®ç¼“å­˜ç›®å½•
        if self.is_vercel:
            # Vercelç¯å¢ƒï¼šä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„é™æ€ç¼“å­˜æ–‡ä»¶
            self.cache_dir = os.path.join(os.getcwd(), "data", "cache")
        else:
            # æœ¬åœ°å¼€å‘ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„
            self.cache_dir = "./data/cache"
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨ï¼ˆæœ¬åœ°å¼€å‘æ—¶ï¼‰
        if not self.is_vercel:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        print(f"ğŸ—ï¸ ç¯å¢ƒæ£€æµ‹: {'Vercel' if self.is_vercel else 'Local'}")
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        
        # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-2.5-pro')
            self.summary_integrator = SummaryIntegrator(api_key, prompts_dir="./prompts")
            self.prompt_manager = PromptManager("./prompts")
        except Exception as e:
            print(f"âš ï¸ åˆå§‹åŒ–APIç»„ä»¶å¤±è´¥: {e}")
            # åœ¨ç¼“å­˜æ¨¡å¼ä¸‹ï¼Œå¯ä»¥ç»§ç»­è¿è¡Œ
            if not cache_only_mode:
                raise e
        
        self.processing_log = {
            "token_count": 0,
            "segments_count": 0,
            "segments_details": [],
            "timestamp_matches": [],
            "processing_time": 0,
            "api_calls": 0,
            "transcription_length": 0,
            "content_type": "",
            "content_subtype": "",
            "confidence": 0.0,
            "environment": "vercel" if self.is_vercel else "local",
            "cache_mode": cache_only_mode
        }
        
        # éªŒè¯å’Œåˆ—å‡ºå¯ç”¨çš„ç¼“å­˜æ–‡ä»¶
        self._validate_cache_setup()

    def _validate_cache_setup(self):
        """éªŒè¯ç¼“å­˜è®¾ç½®å’Œå¯ç”¨æ–‡ä»¶"""
        print(f"\nğŸ” éªŒè¯ç¼“å­˜è®¾ç½®...")
        print(f"ğŸ“‚ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ“‚ ç›®å½•å­˜åœ¨: {os.path.exists(self.cache_dir)}")
        
        if os.path.exists(self.cache_dir):
            cache_files = self._get_available_cache_files()
            print(f"ğŸ“„ æ‰¾åˆ° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶:")
            for i, cache_file in enumerate(cache_files, 1):
                file_path = os.path.join(self.cache_dir, cache_file)
                file_size = os.path.getsize(file_path) / 1024  # KB
                mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                print(f"  {i}. {cache_file}")
                print(f"     å¤§å°: {file_size:.1f}KB")
                print(f"     ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
                
                # éªŒè¯æ–‡ä»¶å†…å®¹
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    has_transcription = 'transcription' in cache_data
                    has_analysis = 'analysis' in cache_data
                    print(f"     å†…å®¹å®Œæ•´æ€§: è½¬å½•={'âœ…' if has_transcription else 'âŒ'}, åˆ†æ={'âœ…' if has_analysis else 'âŒ'}")
                except Exception as e:
                    print(f"     âš ï¸ æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
        else:
            print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")

    def _get_available_cache_files(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶"""
        if not os.path.exists(self.cache_dir):
            return []
        
        try:
            import glob
            pattern = os.path.join(self.cache_dir, "*_analysis.json")
            cache_files = glob.glob(pattern)
            return [os.path.basename(f) for f in cache_files]
        except Exception as e:
            print(f"âš ï¸ æ‰«æç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def _find_best_cache_file(self, lecture_title: str) -> Optional[str]:
        """
        æ ¹æ®è¯¾ç¨‹æ ‡é¢˜æ‰¾åˆ°æœ€åŒ¹é…çš„ç¼“å­˜æ–‡ä»¶
        """
        cache_files = self._get_available_cache_files()
        
        if not cache_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¼“å­˜æ–‡ä»¶")
            return None
        
        print(f"ğŸ” æœç´¢æœ€åŒ¹é…çš„ç¼“å­˜æ–‡ä»¶ï¼Œè¯¾ç¨‹æ ‡é¢˜: {lecture_title}")
        
        # ç­–ç•¥1: å®Œå…¨åŒ¹é…æ ‡é¢˜
        if lecture_title and lecture_title != "Untitled Video":
            for cache_file in cache_files:
                if lecture_title.lower().replace(" ", "_") in cache_file.lower():
                    print(f"âœ… æ‰¾åˆ°æ ‡é¢˜åŒ¹é…çš„ç¼“å­˜æ–‡ä»¶: {cache_file}")
                    return os.path.join(self.cache_dir, cache_file)
        
        # ç­–ç•¥2: ä½¿ç”¨æœ€æ–°çš„ç¼“å­˜æ–‡ä»¶
        if cache_files:
            latest_file = None
            latest_time = 0
            
            for cache_file in cache_files:
                file_path = os.path.join(self.cache_dir, cache_file)
                try:
                    mtime = os.path.getmtime(file_path)
                    if mtime > latest_time:
                        latest_time = mtime
                        latest_file = file_path
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è·å–æ–‡ä»¶æ—¶é—´: {cache_file}, {e}")
                    continue
            
            if latest_file:
                print(f"âœ… ä½¿ç”¨æœ€æ–°çš„ç¼“å­˜æ–‡ä»¶: {os.path.basename(latest_file)}")
                return latest_file
        
        # ç­–ç•¥3: ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ–‡ä»¶
        first_file = os.path.join(self.cache_dir, cache_files[0])
        print(f"âœ… ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç¼“å­˜æ–‡ä»¶: {cache_files[0]}")
        return first_file

    def _load_cache_data(self, cache_file_path: str) -> Optional[Tuple[Dict, Dict]]:
        """
        å®‰å…¨åœ°åŠ è½½ç¼“å­˜æ•°æ®
        """
        if not os.path.exists(cache_file_path):
            print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file_path}")
            return None
        
        try:
            print(f"ğŸ“– æ­£åœ¨è¯»å–ç¼“å­˜æ–‡ä»¶: {os.path.basename(cache_file_path)}")
            
            with open(cache_file_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # éªŒè¯ç¼“å­˜æ•°æ®ç»“æ„
            if 'transcription' not in cache_data:
                print("âŒ ç¼“å­˜æ–‡ä»¶ç¼ºå°‘transcriptionæ•°æ®")
                return None
                
            if 'analysis' not in cache_data:
                print("âŒ ç¼“å­˜æ–‡ä»¶ç¼ºå°‘analysisæ•°æ®")
                return None
            
            transcription = cache_data['transcription']
            analysis = cache_data['analysis']
            
            # éªŒè¯å…³é”®å­—æ®µ
            if not transcription.get('text'):
                print("âš ï¸ è½¬å½•æ–‡æœ¬ä¸ºç©º")
            
            if not analysis.get('content_segments'):
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å†…å®¹ç‰‡æ®µ")
            
            print(f"âœ… æˆåŠŸåŠ è½½ç¼“å­˜æ•°æ®:")
            print(f"  - è½¬å½•æ–‡æœ¬é•¿åº¦: {len(transcription.get('text', ''))} å­—ç¬¦")
            print(f"  - å†…å®¹ç‰‡æ®µæ•°é‡: {len(analysis.get('content_segments', []))}")
            print(f"  - å†…å®¹ç±»å‹: {analysis.get('content_type', 'unknown')}")
            
            return transcription, analysis
            
        except json.JSONDecodeError as e:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def _process_from_cache_only(self, lecture_title: str, language: str = "ä¸­æ–‡") -> Dict:
        """
        ç¼“å­˜æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„å¤„ç†çš„ç¼“å­˜æ•°æ®ç”ŸæˆSummary - æ”¹è¿›ç‰ˆæœ¬
        """
        from datetime import datetime
        start_time = datetime.now()
        
        print("ğŸ”§ ç¼“å­˜æ¨¡å¼ï¼šåŠ è½½é¢„å¤„ç†çš„ç¼“å­˜æ•°æ®...")
        
        # æŸ¥æ‰¾æœ€ä½³ç¼“å­˜æ–‡ä»¶
        cache_file_path = self._find_best_cache_file(lecture_title)
        
        if not cache_file_path:
            error_msg = "ç¼“å­˜æ¨¡å¼ï¼šæœªæ‰¾åˆ°å¯ç”¨çš„ç¼“å­˜æ–‡ä»¶"
            print(f"âŒ {error_msg}")
            
            # æä¾›è°ƒè¯•ä¿¡æ¯
            debug_info = {
                "cache_directory": self.cache_dir,
                "directory_exists": os.path.exists(self.cache_dir),
                "available_files": [],
                "search_title": lecture_title
            }
            
            if os.path.exists(self.cache_dir):
                try:
                    debug_info["available_files"] = os.listdir(self.cache_dir)
                except Exception as e:
                    debug_info["directory_read_error"] = str(e)
            
            return {
                "error": error_msg,
                "success": False,
                "debug_info": debug_info,
                "processor_version": "cache_only_failed"
            }
        
        # åŠ è½½ç¼“å­˜æ•°æ®
        cache_result = self._load_cache_data(cache_file_path)
        
        if not cache_result:
            return {
                "error": "ç¼“å­˜æ¨¡å¼ï¼šç¼“å­˜æ–‡ä»¶æ•°æ®æŸåæˆ–æ— æ³•è¯»å–",
                "success": False,
                "cache_file": os.path.basename(cache_file_path),
                "processor_version": "cache_only_failed"
            }
        
        transcription, analysis = cache_result
        
        # æ›´æ–°å¤„ç†æ—¥å¿—
        self.processing_log.update({
            'segments_count': len(analysis.get('content_segments', [])),
            'content_type': analysis.get('content_type', ''),
            'content_subtype': analysis.get('content_subtype', ''),
            'confidence': analysis.get('confidence', 0.0),
            'transcription_length': len(transcription.get('text', '')),
            'cache_used': True,
            'cache_file': os.path.basename(cache_file_path)
        })
        
        # ç”ŸæˆSummary
        print("ğŸ“„ æ­£åœ¨ç”ŸæˆSummary...")
        try:
            summary_result = self._generate_summary_safely(
                analysis, transcription, lecture_title, language
            )
            
            integrated_summary = summary_result.get("summary", "")
            timestamp_mapping = summary_result.get("timestamp_mapping", {})
            knowledge_points = summary_result.get("knowledge_points", [])
            
        except Exception as e:
            print(f"âš ï¸ Summaryç”Ÿæˆå¤±è´¥: {e}")
            # ç”Ÿæˆå¤‡ç”¨Summary
            integrated_summary, timestamp_mapping, knowledge_points = self._create_fallback_summary(
                analysis, lecture_title
            )
        
        # ç”ŸæˆçŸ¥è¯†ç‚¹ï¼ˆå¦‚æœä¸ºç©ºï¼‰
        if not knowledge_points:
            knowledge_points = self._extract_knowledge_points_from_analysis(analysis)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        self.processing_log["processing_time"] = processing_time
        
        print(f"âœ… ç¼“å­˜æ¨¡å¼å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
        
        return {
            "transcription": transcription,
            "analysis": analysis,
            "notes": "ç¼“å­˜æ¨¡å¼ï¼šåˆ†æ®µç¬”è®°åŠŸèƒ½æš‚æœªå®ç°",
            "summary": "ç¼“å­˜æ¨¡å¼ï¼šåˆ†æ®µæ‘˜è¦åŠŸèƒ½æš‚æœªå®ç°",
            "summary_with_timestamps": "ç¼“å­˜æ¨¡å¼ï¼šå¸¦æ—¶é—´æˆ³çš„åˆ†æ®µæ‘˜è¦åŠŸèƒ½æš‚æœªå®ç°",
            
            # æ–°å¢æ•´åˆSummaryç›¸å…³å­—æ®µ
            "integrated_summary": integrated_summary,
            "timestamp_mapping": timestamp_mapping,
            "knowledge_points": knowledge_points,
            "summary_statistics": {
                "summary_length": len(integrated_summary),
                "knowledge_points_count": len(knowledge_points),
                "segments_count": len(analysis.get('content_segments', [])),
                "processing_mode": "cache_only"
            },
            
            # ç¼“å­˜æ¨¡å¼ç‰¹æœ‰å­—æ®µ
            "cache_used": True,
            "cache_file": os.path.basename(cache_file_path),
            "processor_version": "cache_only",
            "processing_mode": "cache_only",
            
            # å‰ç«¯æœŸæœ›çš„å­—æ®µ
            "lecture_title": lecture_title,
            "language": language,
            "processing_log": self.processing_log,
            "success": True
        }

    def _generate_summary_safely(self, analysis: Dict, transcription: Dict, 
                                lecture_title: str, language: str) -> Dict:
        """
        å®‰å…¨åœ°ç”ŸæˆSummaryï¼Œå¸¦æœ‰é”™è¯¯å¤„ç†
        """
        try:
            if hasattr(self, 'summary_integrator') and self.summary_integrator:
                return self.summary_integrator.generate_summary(
                    analysis, transcription, lecture_title, language
                )
            else:
                print("âš ï¸ SummaryIntegratoræœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self._create_simple_summary(analysis, lecture_title, language)
                
        except Exception as e:
            print(f"âš ï¸ Summaryç”Ÿæˆå¼‚å¸¸: {e}")
            return self._create_simple_summary(analysis, lecture_title, language)

    def _create_simple_summary(self, analysis: Dict, lecture_title: str, language: str) -> Dict:
        """
        åˆ›å»ºç®€å•çš„Summaryï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰
        """
        segments = analysis.get('content_segments', [])
        
        if language.lower() in ["english", "en"]:
            summary = f"# Course Summary: {lecture_title}\n\n"
            summary += "## Course Overview\n"
            summary += f"This video contains {len(segments)} main content segments covering various topics.\n\n"
            summary += "## Main Knowledge Points\n"
        else:
            summary = f"# è¯¾ç¨‹æ€»ç»“: {lecture_title}\n\n"
            summary += "## è¯¾ç¨‹æ¦‚è§ˆ\n"
            summary += f"æœ¬è§†é¢‘åŒ…å«{len(segments)}ä¸ªä¸»è¦å†…å®¹ç‰‡æ®µï¼Œæ¶µç›–å¤šä¸ªä¸»é¢˜ã€‚\n\n"
            summary += "## ä¸»è¦çŸ¥è¯†ç‚¹\n"
        
        for i, segment in enumerate(segments, 1):
            title = segment.get('title', f'ç‰‡æ®µ{i}')
            description = segment.get('description', 'æš‚æ— æè¿°')
            summary += f"{i}. **{title}**: {description}\n"
        
        return {
            "summary": summary,
            "timestamp_mapping": {},
            "knowledge_points": []
        }

    def _create_fallback_summary(self, analysis: Dict, lecture_title: str) -> Tuple[str, Dict, List]:
        """
        åˆ›å»ºå¤‡ç”¨Summaryã€æ—¶é—´æˆ³æ˜ å°„å’ŒçŸ¥è¯†ç‚¹
        """
        segments = analysis.get('content_segments', [])
        
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬æ‘˜è¦
        summary = f"# è¯¾ç¨‹æ€»ç»“: {lecture_title}\n\n"
        summary += "## è¯¾ç¨‹æ¦‚è§ˆ\n"
        summary += f"æœ¬è§†é¢‘åŒ…å«{len(segments)}ä¸ªä¸»è¦å†…å®¹ç‰‡æ®µã€‚ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†çš„AIæ€»ç»“ã€‚\n\n"
        summary += "## ä¸»è¦å†…å®¹ç‰‡æ®µ\n"
        
        timestamp_mapping = {}
        knowledge_points = []
        
        for i, segment in enumerate(segments, 1):
            title = segment.get('title', f'çŸ¥è¯†ç‚¹{i}')
            description = segment.get('description', 'æš‚æ— æè¿°')
            
            summary += f"{i}. **{title}**: {description}\n"
            
            # åˆ›å»ºæ—¶é—´æˆ³æ˜ å°„
            if title:
                timestamp_mapping[title] = {
                    'start_time': segment.get('start_time', '00:00:00'),
                    'end_time': segment.get('end_time', '00:00:00'),
                    'start_seconds': segment.get('start_seconds', 0),
                    'end_seconds': segment.get('end_seconds', 0),
                    'duration_seconds': segment.get('duration_seconds', 0),
                    'description': description
                }
            
            # åˆ›å»ºçŸ¥è¯†ç‚¹
            knowledge_points.append({
                'id': f'kp_{i:03d}',
                'title': title,
                'description': description,
                'start_time': segment.get('start_time', '00:00:00'),
                'end_time': segment.get('end_time', '00:00:00'),
                'category': segment.get('category', 'æ¦‚å¿µ'),
                'difficulty': segment.get('difficulty', 'åŸºç¡€'),
                'importance': segment.get('importance', 'medium')
            })
        
        summary += "\nè¯·æŸ¥çœ‹ä¸‹æ–¹çš„çŸ¥è¯†ç‚¹åˆ—è¡¨è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
        
        return summary, timestamp_mapping, knowledge_points

    def _extract_knowledge_points_from_analysis(self, analysis: Dict) -> List[Dict]:
        """
        ä»analysisä¸­æå–çŸ¥è¯†ç‚¹åˆ—è¡¨
        """
        segments = analysis.get('content_segments', [])
        knowledge_points = []
        
        for i, segment in enumerate(segments):
            kp = {
                'id': segment.get('id', f'kp_{i+1:03d}'),
                'title': segment.get('title', f'çŸ¥è¯†ç‚¹{i+1}'),
                'description': segment.get('description', 'æš‚æ— æè¿°'),
                'start_time': segment.get('start_time', '00:00:00'),
                'end_time': segment.get('end_time', '00:00:00'),
                'key_phrase': segment.get('key_phrase', ''),
                'importance': segment.get('importance', 'medium'),
                'category': segment.get('category', 'æ¦‚å¿µ'),
                'difficulty': segment.get('difficulty', 'åŸºç¡€'),
                'start_seconds': segment.get('start_seconds', 0),
                'end_seconds': segment.get('end_seconds', 0),
                'duration_seconds': segment.get('duration_seconds', 0)
            }
            knowledge_points.append(kp)
        
        return knowledge_points

    def process_video(self, video_path: str, lecture_title: str, language: str = "ä¸­æ–‡") -> Dict:
        """
        å¤„ç†è§†é¢‘çš„å®Œæ•´æµç¨‹ - Vercelä¼˜åŒ–ç‰ˆæœ¬
        """
        print("ğŸš€ å¼€å§‹è§†é¢‘å¤„ç†æµç¨‹...")
        print(f"æ ‡é¢˜: {lecture_title}")
        print(f"ç¼“å­˜æ¨¡å¼: {'å¯ç”¨' if self.cache_only_mode else 'ç¦ç”¨'}")
        print(f"ç¯å¢ƒ: {'Vercel' if self.is_vercel else 'Local'}")
        
        # Vercelç¯å¢ƒæˆ–ç¼“å­˜æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®
        if self.cache_only_mode or self.is_vercel:
            return self._process_from_cache_only(lecture_title, language)
        
        # æœ¬åœ°ç¯å¢ƒï¼šæ­£å¸¸å¤„ç†æµç¨‹
        # ... (ä¿ç•™åŸæœ‰çš„å®Œæ•´å¤„ç†é€»è¾‘)
        return {"error": "æœ¬åœ°å®Œæ•´å¤„ç†æ¨¡å¼æš‚æœªåœ¨æ­¤ç‰ˆæœ¬ä¸­å®ç°"}

    # ä»åŸæ–‡ä»¶ä¿ç•™çš„å…¶ä»–é‡è¦æ–¹æ³•
    def extract_audio_from_video(self, video_path: str, output_audio_path: str) -> str:
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        try:
            import ffmpeg
            (
                ffmpeg
                .input(video_path)
                .output(output_audio_path, acodec='pcm_s16le', ac=1, ar='16k')
                .overwrite_output()
                .run(quiet=True)
            )
            return output_audio_path
        except Exception as e:
            print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            raise e

    def _format_timestamp(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºHH:MM:SSæ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def transcribe_video_with_timestamps(self, video_path: str, model_size: str = "base") -> Dict:
        """ä½¿ç”¨Whisperè½¬å½•è§†é¢‘å¹¶è·å–æ—¶é—´æˆ³"""
        try:
            import whisper
            model = whisper.load_model(model_size)
            result = model.transcribe(video_path, word_timestamps=True)
            
            # è®°å½•è½¬å½•ä¿¡æ¯
            self._log_processing_info("transcription", {
                "text_length": len(result.get('text', '')),
                "segments_count": len(result.get('segments', [])),
                "model_size": model_size
            })
            return result
        except Exception as e:
            print(f"âŒ è§†é¢‘è½¬å½•å¤±è´¥: {e}")
            raise e

    def _find_matching_timestamps(self, start_phrase: str, end_phrase: str, key_phrase: str, transcription: Dict) -> Dict:
        """åŸºäºå…³é”®å¥å­ç²¾ç¡®åŒ¹é…æ—¶é—´æˆ³ - ç®€åŒ–ç‰ˆæœ¬"""
        try:
            segments = transcription.get('segments', [])
            if not segments:
                return {"start_time": "00:00:00", "end_time": "00:00:00", "duration_seconds": 0}
            
            # ç®€åŒ–çš„æ—¶é—´æˆ³åŒ¹é…é€»è¾‘
            start_time = segments[0]['start']
            end_time = segments[-1]['end']
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘ï¼Œä½†ä¸ºäº†Verceléƒ¨ç½²ç®€åŒ–å¤„ç†
            duration = end_time - start_time
            
            return {
                "start_time": self._format_timestamp(start_time),
                "end_time": self._format_timestamp(end_time),
                "duration_seconds": duration,
                "start_seconds": start_time,
                "end_seconds": end_time,
                "match_method": "simplified"
            }
            
        except Exception as e:
            print(f"âŒ æ—¶é—´æˆ³åŒ¹é…å¤±è´¥: {e}")
            return {"start_time": "00:00:00", "end_time": "00:00:00", "duration_seconds": 0}

    def analyze_video_content(self, transcription: Dict, lecture_title: str) -> Dict:
        """åˆ†æè§†é¢‘å†…å®¹ - Vercelä¼˜åŒ–ç‰ˆæœ¬"""
        if self.cache_only_mode or self.is_vercel:
            print("âš ï¸ ç¼“å­˜æ¨¡å¼ä¸‹è·³è¿‡è§†é¢‘å†…å®¹åˆ†æ")
            return {
                "content_type": "æœªçŸ¥",
                "content_subtype": "æœªçŸ¥",
                "confidence": 0.0,
                "content_segments": [],
                "summary": "ç¼“å­˜æ¨¡å¼ä¸‹è·³è¿‡åˆ†æ"
            }
        
        # å¦‚æœä¸æ˜¯ç¼“å­˜æ¨¡å¼ï¼Œæ‰§è¡Œæ­£å¸¸çš„åˆ†ææµç¨‹
        try:
            prompt = self.prompt_manager.get_prompt(
                "video_analysis",
                lecture_title=lecture_title,
                transcription_text=transcription['text']
            )
            
            response = self.model.generate_content(prompt)
            
            if not response or not hasattr(response, 'text') or not response.text:
                raise Exception("APIå“åº”ä¸ºç©ºæˆ–æ— æ•ˆ")
            
            # è§£æç»“æ„åŒ–å“åº”
            result = self._parse_structured_response(response.text)
            return result
            
        except Exception as e:
            print(f"âŒ è§†é¢‘å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {
                "content_type": "åˆ†æå¤±è´¥",
                "content_subtype": "åˆ†æå¤±è´¥",
                "confidence": 0.0,
                "content_segments": [],
                "summary": f"åˆ†æå¤±è´¥: {str(e)}"
            }

    def _parse_structured_response(self, text: str) -> Dict:
        """è§£æç»“æ„åŒ–æ–‡æœ¬å“åº”"""
        # ä¿ç•™åŸæœ‰çš„è§£æé€»è¾‘
        result = {
            "content_type": "æœªçŸ¥",
            "content_subtype": "æœªçŸ¥", 
            "confidence": 0.0,
            "content_segments": [],
            "summary": ""
        }
        
        lines = text.strip().split('\n')
        current_segment = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # è§£æå†…å®¹ç±»å‹
            if line.startswith('å†…å®¹ç±»å‹:') or line.startswith('Content Type:'):
                result["content_type"] = line.split(':', 1)[1].strip()
            elif line.startswith('å†…å®¹å­ç±»å‹:') or line.startswith('Content Subtype:'):
                result["content_subtype"] = line.split(':', 1)[1].strip()
            elif line.startswith('ç½®ä¿¡åº¦:') or line.startswith('Confidence:'):
                try:
                    confidence_str = line.split(':', 1)[1].strip()
                    result["confidence"] = float(confidence_str)
                except:
                    result["confidence"] = 0.0
            elif line.startswith('æ€»ç»“:') or line.startswith('Summary:'):
                result["summary"] = line.split(':', 1)[1].strip()
            elif (line.startswith('ç‰‡æ®µ') or line.startswith('Segment')) and ':' in line:
                # å¼€å§‹æ–°ç‰‡æ®µ
                if current_segment:
                    result["content_segments"].append(current_segment)
                current_segment = {
                    "id": f"seg_{len(result['content_segments'])+1:03d}",
                    "title": "",
                    "description": "",
                    "start_phrase": "",
                    "end_phrase": "",
                    "key_phrase": "",
                    "importance": "",
                    "category": "",
                    "difficulty": ""
                }
            elif current_segment and ':' in line:
                # è§£æç‰‡æ®µå±æ€§
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                if key in ['æ ‡é¢˜', 'Title']:
                    current_segment["title"] = value
                elif key in ['æè¿°', 'Description']:
                    current_segment["description"] = value
                elif key in ['å¼€å§‹å¥', 'Start Sentence']:
                    current_segment["start_phrase"] = value
                elif key in ['ç»“æŸå¥', 'End Sentence']:
                    current_segment["end_phrase"] = value
                elif key in ['å…³é”®å¥', 'Key Sentence']:
                    current_segment["key_phrase"] = value
                elif key in ['é‡è¦æ€§', 'Importance']:
                    current_segment["importance"] = value
                elif key in ['ç±»åˆ«', 'Category']:
                    current_segment["category"] = value
                elif key in ['éš¾åº¦', 'Difficulty']:
                    current_segment["difficulty"] = value
        
        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment:
            result["content_segments"].append(current_segment)
            
        return result

    def _log_processing_info(self, stage: str, info: Dict):
        """è®°å½•å¤„ç†ä¿¡æ¯"""
        print(f"ğŸ“ [{stage}] {info}")
        if stage == "transcription":
            self.processing_log["transcription_length"] = info.get("text_length", 0)
        elif stage == "content_analysis":
            self.processing_log["token_count"] = info.get("token_count", 0)
            self.processing_log["api_calls"] += 1
            self.processing_log["content_type"] = info.get("content_type", "")
            self.processing_log["content_subtype"] = info.get("content_subtype", "")
            self.processing_log["confidence"] = info.get("confidence", 0.0)

    def clear_cache(self, lecture_title: str = None):
        """æ¸…ç†ç¼“å­˜æ–‡ä»¶"""
        if self.is_vercel:
            print("âš ï¸ Vercelç¯å¢ƒä¸‹æ— æ³•æ¸…ç†ç¼“å­˜æ–‡ä»¶ï¼ˆåªè¯»æ–‡ä»¶ç³»ç»Ÿï¼‰")
            return
        
        if lecture_title:
            # æ¸…ç†ç‰¹å®šè¯¾ç¨‹çš„ç¼“å­˜
            pattern = f"*{lecture_title}*_analysis.json"
            import glob
            cache_files = glob.glob(os.path.join(self.cache_dir, pattern))
            for cache_file in cache_files:
                os.remove(cache_file)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¼“å­˜: {cache_file}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            import glob
            cache_files = glob.glob(os.path.join(self.cache_dir, "*_analysis.json"))
            for cache_file in cache_files:
                os.remove(cache_file)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¼“å­˜: {cache_file}")

    def list_cache_files(self):
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶åŠå…¶è¯¦ç»†ä¿¡æ¯"""
        cache_files = self._get_available_cache_files()
        print("ğŸ“‚ ç¼“å­˜æ–‡ä»¶åˆ—è¡¨:")
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ“Š æ–‡ä»¶æ•°é‡: {len(cache_files)}")
        
        if not cache_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¼“å­˜æ–‡ä»¶")
            return
        
        for i, cache_file in enumerate(cache_files, 1):
            file_path = os.path.join(self.cache_dir, cache_file)
            try:
                file_size = os.path.getsize(file_path) / 1024  # KB
                mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                print(f"  {i}. {cache_file}")
                print(f"     å¤§å°: {file_size:.1f}KB")
                print(f"     ä¿®æ”¹æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
                
                # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                with open(file_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                has_transcription = 'transcription' in cache_data and cache_data['transcription'].get('text')
                has_analysis = 'analysis' in cache_data and cache_data['analysis'].get('content_segments')
                
                print(f"     å®Œæ•´æ€§: {'âœ…' if has_transcription and has_analysis else 'âŒ'}")
                
                if has_analysis:
                    segments_count = len(cache_data['analysis'].get('content_segments', []))
                    content_type = cache_data['analysis'].get('content_type', 'unknown')
                    print(f"     å†…å®¹: {segments_count}ä¸ªç‰‡æ®µ, ç±»å‹: {content_type}")
                
            except Exception as e:
                print(f"     âŒ é”™è¯¯: {e}")
        
        print()