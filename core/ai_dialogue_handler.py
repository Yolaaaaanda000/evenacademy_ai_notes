"""
AIå¯¹è¯å¤„ç†å™¨æ¨¡å—
æä¾›ç»Ÿä¸€çš„AIå¯¹è¯åŠŸèƒ½ï¼Œæ”¯æŒä¸åŒä¸Šä¸‹æ–‡ç±»å‹
"""

import google.generativeai as genai
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv


class AIDialogueHandler:
    """AIå¯¹è¯å¤„ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIå¯¹è¯å¤„ç†å™¨"""
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv('llm.env')
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        # è®¾ç½®ä»£ç†ï¼ˆä¸ä¸»ç¨‹åºä¿æŒä¸€è‡´ï¼‰
        os.environ['https_proxy'] = "http://127.0.0.1:8118"
        os.environ['http_proxy'] = "http://127.0.0.1:8118"
        os.environ['all_proxy'] = "socks5://127.0.0.1:8119"
        
        self.model = genai.GenerativeModel('gemini-2.5-pro')
        self.prompt_templates = self._load_prompt_templates()
    
    def _load_prompt_templates(self) -> Dict[str, str]:
        """ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½æç¤ºè¯æ¨¡æ¿"""
        try:
            import re
            
            # è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶
            template_file_path = 'prompts/ai_dialogue_templates.md'
            
            if not os.path.exists(template_file_path):
                print(f"âš ï¸  æç¤ºè¯æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_file_path}")
                return self._get_default_templates()
            
            with open(template_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£ææ¨¡æ¿
            templates = {}
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ¨¡æ¿
            template_pattern = r'## ([^\n]+) Context Template \(([^)]+)\)\n\n```\n(.*?)\n```'
            matches = re.findall(template_pattern, content, re.DOTALL)
            
            for title, context_type, template in matches:
                templates[context_type] = template.strip()
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(templates)} ä¸ªæç¤ºè¯æ¨¡æ¿")
            return templates
            
        except Exception as e:
            print(f"âŒ åŠ è½½æç¤ºè¯æ¨¡æ¿å¤±è´¥: {e}")
            return self._get_default_templates()
    
    def _get_default_templates(self) -> Dict[str, str]:
        """Get default prompt templates (fallback)"""
        return {
            'video': """
You are a professional AI learning assistant, specialized in helping students understand video course content.

Video Information:
- Title: {title}
- Current playback time: {current_time} seconds
- Total duration: {duration} seconds

Knowledge Points:
{knowledge_points}

Video Summary:
{summary}

Student Question: {message}

Dialogue History:
{dialogue_history}

Please provide helpful answers based on the video content and the student's specific questions. Your responses should:
1. Directly address the student's question
2. Incorporate specific knowledge points from the video
3. Provide relevant explanations and examples
4. Encourage deep thinking
5. Use friendly and understandable language

Please respond in English.
            """,
            
            'summary': """
You are a professional AI summary assistant, specialized in helping students understand course summary content.

Summary Information:
- Title: {title}
- Content: {content}
- Concept Count: {concept_count}

Student Question: {message}

Dialogue History:
{dialogue_history}

Please provide professional, accurate, and helpful answers based on the summary content and the student's specific questions. Your responses should:
1. Be based on the actual content in the summary
2. Use clear and concise language
3. Provide specific explanations and examples
4. Encourage deep thinking
5. Reference specific knowledge points when possible

Please respond in English.
            """,
            
            'practice': """
You are a professional AI practice assistant, specialized in helping students solve practice problems.

Practice Information:
- Knowledge Point: {knowledge_point}
- Current Question: {current_question}

Student Question: {message}

Dialogue History:
{dialogue_history}

Please provide helpful answers based on the practice content and the student's specific questions. Your responses should:
1. Directly address the student's question
2. Provide problem-solving strategies and techniques
3. Analyze error causes (if any)
4. Give improvement suggestions
5. Use encouraging language

Please respond in English.
            """,
            
            'knowledge_graph': """
You are a professional AI knowledge assistant, specialized in helping students understand knowledge graphs.

Knowledge Graph Information:
- Title: {title}
- Concept Relationships: {concept_relationships}
- Knowledge Structure: {knowledge_structure}

Student Question: {message}

Dialogue History:
{dialogue_history}

Please provide helpful answers based on the knowledge graph content and the student's specific questions. Your responses should:
1. Explain relationships between concepts
2. Provide learning path suggestions
3. Analyze knowledge structure
4. Encourage exploration
5. Use clear language

Please respond in English.
            """
        }
    
    def generate_response(self, context_type: str, message: str, context_data: Dict[str, Any], 
                         dialogue_history: List[Dict[str, Any]] = None) -> str:
        """
        ç”ŸæˆAIå“åº”
        
        Args:
            context_type: ä¸Šä¸‹æ–‡ç±»å‹ (video, summary, practice, knowledge_graph)
            message: ç”¨æˆ·æ¶ˆæ¯
            context_data: ä¸Šä¸‹æ–‡æ•°æ®
            dialogue_history: å¯¹è¯å†å²
            
        Returns:
            str: AIå“åº”å†…å®¹
        """
        try:
            print(f"ğŸ¤– [LLMè°ƒç”¨å¼€å§‹] ä¸Šä¸‹æ–‡ç±»å‹: {context_type}")
            print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯: {message}")
            
            # è·å–æç¤ºè¯æ¨¡æ¿
            template = self.prompt_templates.get(context_type, self.prompt_templates['video'])
            print(f"ğŸ“‹ ä½¿ç”¨æ¨¡æ¿: {context_type}")
            
            # æ ¼å¼åŒ–å¯¹è¯å†å²
            history_text = self._format_dialogue_history(dialogue_history or [])
            print(f"ğŸ’¬ å¯¹è¯å†å²é•¿åº¦: {len(history_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡æ•°æ®
            formatted_context = self._format_context_data(context_type, context_data)
            print(f"ğŸ“Š ä¸Šä¸‹æ–‡æ•°æ®å­—æ®µ: {list(formatted_context.keys())}")
            
            # æ„å»ºå®Œæ•´æç¤ºè¯
            prompt = template.format(
                message=message,
                dialogue_history=history_text,
                **formatted_context
            )
            print(f"ğŸ“„ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # è°ƒç”¨AIæ¨¡å‹
            print(f"ğŸš€ å¼€å§‹è°ƒç”¨Gemini API...")
            response = self.model.generate_content(prompt)
            
            # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹
            if not response:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: å“åº”ä¸ºç©º")
                return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰finish_reasoné”™è¯¯
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'finish_reason'):
                    finish_reason = candidate.finish_reason
                    if finish_reason in [0, 1]:  # 0å’Œ1éƒ½è¡¨ç¤ºæ­£å¸¸å®Œæˆ
                        print(f"âœ… LLMè°ƒç”¨æ­£å¸¸å®Œæˆ (finish_reason={finish_reason})")
                    elif finish_reason == 2:
                        print(f"âš ï¸ LLMè°ƒç”¨è¾¾åˆ°æœ€å¤§tokené™åˆ¶ (finish_reason=2)")
                    elif finish_reason == 3:
                        print(f"âŒ LLMè°ƒç”¨è¢«å®‰å…¨è¿‡æ»¤é˜»æ­¢ (finish_reason=3)")
                        return "æŠ±æ­‰ï¼Œå†…å®¹å› å®‰å…¨é—®é¢˜è¢«é˜»æ­¢ï¼Œè¯·é‡æ–°æé—®ã€‚"
                    elif finish_reason == 4:
                        print(f"âš ï¸ LLMè°ƒç”¨è¾¾åˆ°é€’å½’é™åˆ¶ (finish_reason=4)")
                    else:
                        print(f"âš ï¸ LLMè°ƒç”¨å‡ºç°æœªçŸ¥çŠ¶æ€ (finish_reason={finish_reason})")
            
            # æ£€æŸ¥å“åº”æ–‡æœ¬
            if not hasattr(response, 'text') or not response.text:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: å“åº”æ²¡æœ‰æ–‡æœ¬å†…å®¹")
                return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            
            print(f"âœ… LLMè°ƒç”¨æˆåŠŸ!")
            print(f"ğŸ“¤ å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            return response.text
            
        except Exception as e:
            print(f"âŒ AIå¯¹è¯å¤„ç†é”™è¯¯: {e}")
            print(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"
    
    def _format_dialogue_history(self, history: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        if not history:
            return "æ— å¯¹è¯å†å²"
        
        formatted = []
        for msg in history[-5:]:  # åªä¿ç•™æœ€è¿‘5æ¡æ¶ˆæ¯
            role = "ç”¨æˆ·" if msg.get('type') == 'user' else "AIåŠ©æ‰‹"
            content = msg.get('content', '')
            timestamp = msg.get('timestamp', '')
            
            if timestamp:
                time_str = timestamp.strftime('%H:%M') if hasattr(timestamp, 'strftime') else str(timestamp)
                formatted.append(f"[{time_str}] {role}: {content}")
            else:
                formatted.append(f"{role}: {content}")
        
        return "\n".join(formatted)
    
    def _format_context_data(self, context_type: str, context_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡æ•°æ®"""
        if context_type == 'video':
            return {
                'title': context_data.get('title', 'æœªçŸ¥è§†é¢‘'),
                'current_time': context_data.get('current_time', 0),
                'duration': context_data.get('duration', 0),
                'knowledge_points': self._format_knowledge_points(context_data.get('knowledge_points', [])),
                'summary': context_data.get('summary', 'æš‚æ— æ‘˜è¦')
            }
        elif context_type == 'summary':
            return {
                'title': context_data.get('title', 'æœªçŸ¥æ‘˜è¦'),
                'content': context_data.get('content', ''),
                'concept_count': context_data.get('concept_count', '0')
            }
        elif context_type == 'practice':
            return {
                'knowledge_point': context_data.get('knowledge_point', ''),
                'current_question': context_data.get('current_question', '')
            }
        elif context_type == 'knowledge_graph':
            return {
                'title': context_data.get('title', 'æœªçŸ¥çŸ¥è¯†å›¾è°±'),
                'concept_relationships': context_data.get('concept_relationships', ''),
                'knowledge_structure': context_data.get('knowledge_structure', '')
            }
        else:
            return context_data
    
    def _format_knowledge_points(self, knowledge_points: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†ç‚¹åˆ—è¡¨"""
        if not knowledge_points:
            return "æš‚æ— çŸ¥è¯†ç‚¹ä¿¡æ¯"
        
        formatted = []
        for i, point in enumerate(knowledge_points, 1):
            title = point.get('title', 'æœªçŸ¥çŸ¥è¯†ç‚¹')
            time = point.get('time', 'æœªçŸ¥æ—¶é—´')
            formatted.append(f"{i}. {title} (æ—¶é—´: {time})")
        
        return "\n".join(formatted)
    
    def validate_request(self, context_type: str, message: str, context_data: Dict[str, Any]) -> bool:
        """éªŒè¯è¯·æ±‚å‚æ•°"""
        if not message or not message.strip():
            return False
        
        if context_type not in self.prompt_templates:
            return False
        
        return True
    
    def get_supported_context_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„ä¸Šä¸‹æ–‡ç±»å‹"""
        return list(self.prompt_templates.keys())


# å…¨å±€AIå¯¹è¯å¤„ç†å™¨å®ä¾‹
ai_dialogue_handler = AIDialogueHandler()


def handle_ai_dialogue_request(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¤„ç†AIå¯¹è¯è¯·æ±‚çš„ç»Ÿä¸€æ¥å£
    
    Args:
        request_data: è¯·æ±‚æ•°æ®
        
    Returns:
        Dict[str, Any]: å“åº”æ•°æ®
    """
    try:
        print(f"\nğŸ¯ [AIå¯¹è¯è¯·æ±‚å¼€å§‹]")
        print(f"ğŸ“¦ è¯·æ±‚æ•°æ®: {list(request_data.keys())}")
        
        # æå–è¯·æ±‚å‚æ•°
        message = request_data.get('message', '')
        context_type = request_data.get('context_type', 'video')
        context_data = request_data.get('context_data', {})
        dialogue_history = request_data.get('dialogue_history', [])
        
        print(f"ğŸ“ æ¶ˆæ¯: {message[:50]}{'...' if len(message) > 50 else ''}")
        print(f"ğŸ·ï¸  ä¸Šä¸‹æ–‡ç±»å‹: {context_type}")
        print(f"ğŸ“Š ä¸Šä¸‹æ–‡æ•°æ®å­—æ®µ: {list(context_data.keys())}")
        print(f"ğŸ’¬ å¯¹è¯å†å²æ¡æ•°: {len(dialogue_history)}")
        
        # éªŒè¯è¯·æ±‚
        if not ai_dialogue_handler.validate_request(context_type, message, context_data):
            print(f"âŒ è¯·æ±‚éªŒè¯å¤±è´¥")
            return {
                'success': False,
                'error': 'è¯·æ±‚å‚æ•°æ— æ•ˆ'
            }
        
        print(f"âœ… è¯·æ±‚éªŒè¯é€šè¿‡")
        
        # ç”Ÿæˆå“åº”
        response = ai_dialogue_handler.generate_response(
            context_type=context_type,
            message=message,
            context_data=context_data,
            dialogue_history=dialogue_history
        )
        
        print(f"ğŸ‰ [AIå¯¹è¯è¯·æ±‚å®Œæˆ] å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        
        return {
            'success': True,
            'response': response
        }
        
    except Exception as e:
        print(f"âŒ å¤„ç†AIå¯¹è¯è¯·æ±‚æ—¶å‡ºé”™: {e}")
        print(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
        return {
            'success': False,
            'error': f'å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'
        } 