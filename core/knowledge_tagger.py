"""
çŸ¥è¯†ç‚¹æ ‡ç­¾å™¨ - ä»Summaryä¸­æå–çŸ¥è¯†ç‚¹å¹¶ä¸é¢˜åº“åŒ¹é…
"""

import re
import json
from typing import List, Dict, Optional, Tuple
from core.question_matcher import QuestionMatcher


class KnowledgeTagger:
    """çŸ¥è¯†ç‚¹æ ‡ç­¾å™¨"""
    
    def __init__(self):
        self.question_matcher = QuestionMatcher()
        self.knowledge_points_cache = {}
        
    def extract_knowledge_points_from_summary(self, summary: str) -> List[Dict]:
        """
        ä»Summaryä¸­æå–çŸ¥è¯†ç‚¹æ ‡ç­¾
        
        Args:
            summary: åŒ…å«[KP:çŸ¥è¯†ç‚¹]æ ‡ç­¾çš„Summaryæ–‡æœ¬
            
        Returns:
            List[Dict]: æå–çš„çŸ¥è¯†ç‚¹åˆ—è¡¨
        """
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… [KP:çŸ¥è¯†ç‚¹] æ ¼å¼
        pattern = r'\[KP:(.*?)\]'
        matches = re.findall(pattern, summary)
        
        knowledge_points = []
        for i, match in enumerate(matches):
            knowledge_point = match.strip()
            if knowledge_point:
                knowledge_points.append({
                    'id': f'kp_{i+1:03d}',
                    'name': knowledge_point,
                    'original_text': f'[KP:{knowledge_point}]',
                    'position': summary.find(f'[KP:{knowledge_point}]'),
                    'context': self._extract_context(summary, knowledge_point)
                })
        
        print(f"ğŸ” ä»Summaryä¸­æå–åˆ° {len(knowledge_points)} ä¸ªçŸ¥è¯†ç‚¹:")
        for kp in knowledge_points:
            print(f"  - {kp['name']}")
        
        return knowledge_points
    
    def _extract_context(self, summary: str, knowledge_point: str) -> str:
        """æå–çŸ¥è¯†ç‚¹å‘¨å›´çš„ä¸Šä¸‹æ–‡"""
        pattern = f'\\[KP:{re.escape(knowledge_point)}\\]'
        match = re.search(pattern, summary)
        if match:
            start = max(0, match.start() - 100)
            end = min(len(summary), match.end() + 100)
            return summary[start:end].strip()
        return ""
    
    def match_questions_for_knowledge_points(self, knowledge_points: List[Dict], 
                                           limit_per_kp: int = 3) -> Dict[str, List[Dict]]:
        """
        ä¸ºæ¯ä¸ªçŸ¥è¯†ç‚¹åŒ¹é…ç›¸å…³é¢˜ç›®
        
        Args:
            knowledge_points: çŸ¥è¯†ç‚¹åˆ—è¡¨
            limit_per_kp: æ¯ä¸ªçŸ¥è¯†ç‚¹æœ€å¤šåŒ¹é…çš„é¢˜ç›®æ•°é‡
            
        Returns:
            Dict[str, List[Dict]]: çŸ¥è¯†ç‚¹åˆ°é¢˜ç›®çš„æ˜ å°„
        """
        matches = {}
        
        for kp in knowledge_points:
            kp_name = kp['name']
            print(f"\nğŸ¯ ä¸ºçŸ¥è¯†ç‚¹ '{kp_name}' åŒ¹é…é¢˜ç›®...")
            
            # ä½¿ç”¨QuestionMatcheråŒ¹é…é¢˜ç›®
            matched_questions = self.question_matcher.find_questions_by_topics(
                predicted_topics=[kp_name], 
                limit=limit_per_kp
            )
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ¹é…ï¼Œå°è¯•å…³é”®è¯åŒ¹é…
            if not matched_questions:
                print(f"  âš ï¸  ç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•å…³é”®è¯åŒ¹é…...")
                matched_questions = self._match_by_keywords(kp_name, limit_per_kp)
            
            matches[kp_name] = matched_questions
            print(f"  âœ… åŒ¹é…åˆ° {len(matched_questions)} é“é¢˜ç›®")
            
            # æ˜¾ç¤ºåŒ¹é…çš„é¢˜ç›®
            for i, question in enumerate(matched_questions[:2]):  # åªæ˜¾ç¤ºå‰2é“
                print(f"    {i+1}. {question.get('formatted_title', 'Unknown')}")
        
        return matches
    
    def _match_by_keywords(self, knowledge_point: str, limit: int) -> List[Dict]:
        """åŸºäºå…³é”®è¯åŒ¹é…é¢˜ç›®"""
        # å®šä¹‰å…³é”®è¯æ˜ å°„
        keyword_mapping = {
            'æ¦‚ç‡': ['probability', 'prob', 'chance', 'random'],
            'ç»Ÿè®¡': ['statistics', 'stat', 'data', 'distribution'],
            'ç»„åˆ': ['combination', 'combinatorics', 'permutation'],
            'å‡ ä½•': ['geometry', 'geometric', 'triangle', 'circle'],
            'ä»£æ•°': ['algebra', 'equation', 'polynomial'],
            'æ•°è®º': ['number theory', 'divisibility', 'prime'],
            'å‡½æ•°': ['function', 'graph', 'domain', 'range']
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„å…³é”®è¯
        matched_keywords = []
        for chinese_keyword, english_keywords in keyword_mapping.items():
            if chinese_keyword in knowledge_point:
                matched_keywords.extend(english_keywords)
        
        if matched_keywords:
            return self.question_matcher.find_questions_by_topics(
                predicted_topics=matched_keywords, 
                limit=limit
            )
        
        return []
    
    def generate_knowledge_point_report(self, knowledge_points: List[Dict], 
                                      question_matches: Dict[str, List[Dict]]) -> Dict:
        """
        ç”ŸæˆçŸ¥è¯†ç‚¹åŒ¹é…æŠ¥å‘Š
        
        Args:
            knowledge_points: çŸ¥è¯†ç‚¹åˆ—è¡¨
            question_matches: é¢˜ç›®åŒ¹é…ç»“æœ
            
        Returns:
            Dict: åŒ¹é…æŠ¥å‘Š
        """
        report = {
            'summary': {
                'total_knowledge_points': len(knowledge_points),
                'total_matched_questions': sum(len(questions) for questions in question_matches.values()),
                'average_questions_per_kp': sum(len(questions) for questions in question_matches.values()) / len(knowledge_points) if knowledge_points else 0
            },
            'knowledge_points': [],
            'question_statistics': self.question_matcher.get_question_statistics()
        }
        
        for kp in knowledge_points:
            kp_name = kp['name']
            matched_questions = question_matches.get(kp_name, [])
            
            kp_report = {
                'id': kp['id'],
                'name': kp_name,
                'context': kp['context'],
                'matched_questions_count': len(matched_questions),
                'questions': matched_questions,
                'difficulty_distribution': self._analyze_difficulty(matched_questions),
                'topic_coverage': self._analyze_topic_coverage(matched_questions)
            }
            
            report['knowledge_points'].append(kp_report)
        
        return report
    
    def _analyze_difficulty(self, questions: List[Dict]) -> Dict:
        """åˆ†æé¢˜ç›®éš¾åº¦åˆ†å¸ƒ"""
        difficulty_counts = {}
        for question in questions:
            # æ ¹æ®é¢˜ç›®ç‰¹å¾åˆ¤æ–­éš¾åº¦
            score = question.get('total_score', 0)
            if score >= 5:
                difficulty = 'hard'
            elif score >= 3:
                difficulty = 'medium'
            else:
                difficulty = 'easy'
            
            difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1
        
        return difficulty_counts
    
    def _analyze_topic_coverage(self, questions: List[Dict]) -> Dict:
        """åˆ†æé¢˜ç›®ä¸»é¢˜è¦†ç›–"""
        topic_counts = {}
        for question in questions:
            topic = question.get('topics', '')
            if topic:
                topic_counts[topic] = topic_counts.get(topic, 0) + 1
        
        return topic_counts
    
    def create_practice_session(self, knowledge_point: str, 
                              questions: List[Dict]) -> Dict:
        """
        ä¸ºæŒ‡å®šçŸ¥è¯†ç‚¹åˆ›å»ºç»ƒä¹ ä¼šè¯
        
        Args:
            knowledge_point: çŸ¥è¯†ç‚¹åç§°
            questions: åŒ¹é…çš„é¢˜ç›®åˆ—è¡¨
            
        Returns:
            Dict: ç»ƒä¹ ä¼šè¯é…ç½®
        """
        session = {
            'knowledge_point': knowledge_point,
            'total_questions': len(questions),
            'questions': [],
            'session_id': f"session_{knowledge_point}_{len(questions)}",
            'created_at': self._get_current_timestamp()
        }
        
        for i, question in enumerate(questions):
            session['questions'].append({
                'id': f"q_{i+1:03d}",
                'problem_id': question.get('Problem ID', ''),
                'title': question.get('formatted_title', ''),
                'question_text': question.get('question_text', ''),
                'topics': question.get('topics', ''),
                'difficulty': self._estimate_difficulty(question),
                'difficulty_level': question.get('difficulty_level', 3),
                'relevance_score': question.get('relevance_score', 0),
                'practice_priority': question.get('practice_priority', 5),
                'expected_match': question.get('expected_match', 'MEDIUM'),
                'recommended_order': question.get('recommended_order', 'standard'),
                # ğŸ†• æ·»åŠ ç¼ºå¤±çš„å…³é”®å­—æ®µ
                'answer': question.get('answer', ''),
                'explanation': question.get('explanation', ''),
                'hint': question.get('hint', ''),
                # ğŸ†• æ·»åŠ é€‰é¡¹å­—æ®µ
                'optionA': question.get('optionA', ''),
                'optionB': question.get('optionB', ''),
                'optionC': question.get('optionC', ''),
                'optionD': question.get('optionD', ''),
                'optionE': question.get('optionE', '')
            })
        
        return session
    
    def _estimate_difficulty(self, question: Dict) -> str:
        """ä¼°ç®—é¢˜ç›®éš¾åº¦"""
        score = question.get('total_score', 0)
        if score >= 5:
            return 'hard'
        elif score >= 3:
            return 'medium'
        else:
            return 'easy'
    
    def _get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def save_matching_results(self, results: Dict, filename: str = None) -> str:
        """
        ä¿å­˜åŒ¹é…ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: åŒ¹é…ç»“æœ
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"knowledge_matching_results_{timestamp}.json"
        
        filepath = f"data/results/{filename}"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        import os
        os.makedirs("data/results", exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ åŒ¹é…ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºçŸ¥è¯†ç‚¹æ ‡ç­¾å™¨
    tagger = KnowledgeTagger()
    
    # ç¤ºä¾‹Summaryæ–‡æœ¬
    sample_summary = """
    æœ¬è¯¾ç¨‹ä»‹ç»äº†æ¦‚ç‡è®ºçš„åŸºç¡€æ¦‚å¿µã€‚
    
    é¦–å…ˆè®²è§£äº†[KP:æ¦‚ç‡çš„ç»Ÿè®¡å­¦è¯ é‡Š]ï¼ŒåŒ…æ‹¬å¤§æ•°å®šå¾‹å’Œé•¿æœŸè¿è¡Œå¹³å‡å€¼çš„æ¦‚å¿µã€‚
    æ¥ç€ä»‹ç»äº†[KP:ä¸¤ç§æ¦‚ç‡ç±»å‹]ï¼Œå³æ ·æœ¬æ¦‚ç‡å’Œå®éªŒæ¦‚ç‡çš„åŒºåˆ«ã€‚
    æœ€åè¯¦ç»†å®šä¹‰äº†[KP:å®éªŒä¸ç»“æœ]è¿™ä¸¤ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚
    """
    
    # æå–çŸ¥è¯†ç‚¹
    knowledge_points = tagger.extract_knowledge_points_from_summary(sample_summary)
    
    # åŒ¹é…é¢˜ç›®
    question_matches = tagger.match_questions_for_knowledge_points(knowledge_points)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tagger.generate_knowledge_point_report(knowledge_points, question_matches)
    
    print("\nğŸ“Š åŒ¹é…æŠ¥å‘Š:")
    print(f"æ€»çŸ¥è¯†ç‚¹æ•°: {report['summary']['total_knowledge_points']}")
    print(f"åŒ¹é…é¢˜ç›®æ•°: {report['summary']['total_matched_questions']}")
    print(f"å¹³å‡æ¯é¢˜çŸ¥è¯†ç‚¹: {report['summary']['average_questions_per_kp']:.1f}") 