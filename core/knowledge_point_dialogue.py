"""
Knowledge Point Dialogue Handler
ä¸“é—¨å¤„ç†å•ä¸ªçŸ¥è¯†ç‚¹çš„æ·±åº¦å¯¹è¯åŠŸèƒ½
"""

import google.generativeai as genai
import os
import re
from datetime import datetime
from typing import Dict, List, Any
from dotenv import load_dotenv


class KnowledgePointDialogueHandler:
    """çŸ¥è¯†ç‚¹ä¸“ç”¨å¯¹è¯å¤„ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†ç‚¹å¯¹è¯å¤„ç†å™¨"""
        # ä¸»ç¨‹åºå·²ç»è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
        self.model = genai.GenerativeModel('gemini-2.5-flash')
        self.prompt_template = self._load_knowledge_point_prompt()
    
    def _load_knowledge_point_prompt(self) -> str:
        """åŠ è½½çŸ¥è¯†ç‚¹ä¸“ç”¨Promptæ¨¡æ¿"""
        prompt_file_path = 'prompts/knowledge_point_focused.md'
        
        if not os.path.exists(prompt_file_path):
            print(f"âš ï¸  çŸ¥è¯†ç‚¹Promptæ–‡ä»¶ä¸å­˜åœ¨: {prompt_file_path}ï¼Œä½¿ç”¨é»˜è®¤Prompt")
            return self._get_default_prompt()
        
        try:
            with open(prompt_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–ä¸»Promptæ¨¡æ¿ï¼ˆåœ¨```ä¹‹é—´çš„å†…å®¹ï¼‰
            prompt_pattern = r'```\n(.*?)\n```'
            matches = re.findall(prompt_pattern, content, re.DOTALL)
            
            if not matches:
                print("âš ï¸  åœ¨Promptæ–‡ä»¶ä¸­æœªæ‰¾åˆ°æ¨¡æ¿å†…å®¹ï¼Œä½¿ç”¨é»˜è®¤Prompt")
                return self._get_default_prompt()
            
            template = matches[0].strip()
            
            # éªŒè¯æ¨¡æ¿ä¸­æ˜¯å¦åŒ…å«å¿…è¦çš„å ä½ç¬¦
            required_placeholders = [
                '{knowledge_point_title}',
                '{knowledge_point_content}',
                '{knowledge_point_timestamp}',
                '{video_title}',
                '{related_concepts}',
                '{user_message}',
                '{dialogue_history}',
                '{dialogue_round}',
                '{focus_deviation_count}',
                '{language}'
            ]
            
            missing_placeholders = []
            for placeholder in required_placeholders:
                if placeholder not in template:
                    missing_placeholders.append(placeholder)
            
            if missing_placeholders:
                print(f"âš ï¸  Promptæ¨¡æ¿ç¼ºå°‘å¿…è¦å ä½ç¬¦: {missing_placeholders}ï¼Œä½¿ç”¨é»˜è®¤Prompt")
                return self._get_default_prompt()
            
            print("âœ… æˆåŠŸåŠ è½½çŸ¥è¯†ç‚¹ä¸“ç”¨Promptæ¨¡æ¿")
            return template
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½Promptæ–‡ä»¶æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤Prompt")
            return self._get_default_prompt()
    
    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤çš„çŸ¥è¯†ç‚¹Promptæ¨¡æ¿"""
        return """
You are a professional knowledge point deep analysis assistant, specialized in helping students deeply understand individual knowledge points.

## Knowledge Point Information
- Knowledge Point Title: {knowledge_point_title}
- Knowledge Point Content: {knowledge_point_content}
- Knowledge Point Timestamp: {knowledge_point_timestamp}
- Source Video: {video_title}
- Related Concepts: {related_concepts}

## Dialogue Rules
1. **Strict Focus**: Only answer questions directly related to "{knowledge_point_title}"
2. **Deep Analysis**: Provide detailed explanations, principles, applications, and examples of the knowledge point
3. **Relevance Judgment**: If user questions exceed the knowledge point scope, politely refuse and guide back to focus
4. **Active Guidance**: Provide targeted deep question suggestions
5. **Knowledge Expansion**: Provide related concepts and practical applications within the knowledge point scope

## Current Dialogue State
- User Question: {user_message}
- Dialogue History: {dialogue_history}
- Dialogue Round: {dialogue_round}
- Focus Deviation Count: {focus_deviation_count}

## Response Requirements
1. First judge question relevance (high/medium/low)
2. Provide corresponding responses based on relevance:
   - High relevance: Deep analysis + Extended content + Guiding questions
   - Medium relevance: Brief answer + Guide back to focus
   - Low relevance: Polite refusal + Provide knowledge point related suggested questions
3. Respond in {language}
4. Maintain friendly and encouraging tone

Please start responding:
"""
    

    
    def generate_response(
        self, 
        knowledge_point_data: Dict[str, Any],
        user_message: str,
        dialogue_history: List[Dict[str, Any]] = None,
        dialogue_state: Dict[str, Any] = None,
        language: str = 'English'
    ) -> str:
        """
        ç”ŸæˆçŸ¥è¯†ç‚¹ä¸“ç”¨å¯¹è¯å›å¤
        
        Args:
            knowledge_point_data: çŸ¥è¯†ç‚¹æ•°æ®
            user_message: ç”¨æˆ·æ¶ˆæ¯
            dialogue_history: å¯¹è¯å†å²
            dialogue_state: å¯¹è¯çŠ¶æ€
            
        Returns:
            str: AIå›å¤å†…å®¹
        """
        try:
            # åˆå§‹åŒ–æˆ–æ›´æ–°å¯¹è¯çŠ¶æ€
            if dialogue_state is None:
                dialogue_state = {
                    'round': 0,
                    'focus_deviation_count': 0
                }
            
            # æ›´æ–°å¯¹è¯è½®æ¬¡
            dialogue_state['round'] += 1
            
            # æ ¼å¼åŒ–å¯¹è¯å†å²
            formatted_history = self._format_dialogue_history(dialogue_history or [])
            
            # å‡†å¤‡Promptå‚æ•°
            prompt_params = {
                'knowledge_point_title': knowledge_point_data.get('title', ''),
                'knowledge_point_content': knowledge_point_data.get('content', ''),
                'knowledge_point_timestamp': knowledge_point_data.get('timestamp', ''),
                'video_title': knowledge_point_data.get('video_title', ''),
                'related_concepts': knowledge_point_data.get('related_concepts', ''),
                'user_message': user_message,
                'dialogue_history': formatted_history,
                'dialogue_round': dialogue_state['round'],
                'focus_deviation_count': dialogue_state['focus_deviation_count'],
                'language': language
            }
            
            # æ ¼å¼åŒ–Prompt
            try:
                formatted_prompt = self.prompt_template.format(**prompt_params)
                print(f"ğŸ“ Promptæ ¼å¼åŒ–æˆåŠŸï¼Œé•¿åº¦: {len(formatted_prompt)} å­—ç¬¦")
            except KeyError as e:
                print(f"âŒ Promptæ ¼å¼åŒ–å¤±è´¥ï¼Œç¼ºå°‘å‚æ•°: {e}")
                return f"æŠ±æ­‰ï¼Œç³»ç»Ÿé…ç½®é”™è¯¯ï¼Œç¼ºå°‘å¿…è¦å‚æ•°: {str(e)}"
            except Exception as e:
                print(f"âŒ Promptæ ¼å¼åŒ–å¤±è´¥: {e}")
                return f"æŠ±æ­‰ï¼Œç³»ç»Ÿé…ç½®é”™è¯¯: {str(e)}"
            
            # ç”Ÿæˆå›å¤
            try:
                response = self.model.generate_content(formatted_prompt)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹
                if not response:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: å“åº”ä¸ºç©º")
                    return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰finish_reasoné”™è¯¯
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = candidate.finish_reason
                        if finish_reason in [0, 1]:  # 0å’Œ1éƒ½è¡¨ç¤ºæ­£å¸¸å®Œæˆ
                            print(f"âœ… LLMè°ƒç”¨æ­£å¸¸å®Œæˆ (finish_reason={finish_reason})")
                        elif finish_reason == 2:
                            print(f"âš ï¸ LLMè°ƒç”¨è¾¾åˆ°æœ€å¤§tokené™åˆ¶ (finish_reason=2)")
                        elif finish_reason == 3:
                            print(f"âŒ LLMè°ƒç”¨è¢«å®‰å…¨è¿‡æ»¤é˜»æ­¢ (finish_reason=3)")
                            return "æŠ±æ­‰ï¼Œå†…å®¹å› å®‰å…¨é—®é¢˜è¢«é˜»æ­¢ï¼Œè¯·é‡æ–°æé—®ã€‚"
                        elif finish_reason == 4:
                            print(f"âš ï¸ LLMè°ƒç”¨è¾¾åˆ°é€’å½’é™åˆ¶ (finish_reason=4)")
                        else:
                            print(f"âš ï¸ LLMè°ƒç”¨å‡ºç°æœªçŸ¥çŠ¶æ€ (finish_reason={finish_reason})")
                
                # æ£€æŸ¥å“åº”æ–‡æœ¬
                if not hasattr(response, 'text') or not response.text:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: å“åº”æ²¡æœ‰æ–‡æœ¬å†…å®¹")
                    return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                
                ai_response = response.text
                print(f"âœ… çŸ¥è¯†ç‚¹å¯¹è¯å›å¤ç”ŸæˆæˆåŠŸ (è½®æ¬¡: {dialogue_state['round']})")
                return ai_response
            except Exception as e:
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
                return f"æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚é”™è¯¯ä¿¡æ¯: {str(e)}"
            
        except Exception as e:
            print(f"âŒ ç”ŸæˆçŸ¥è¯†ç‚¹å¯¹è¯å›å¤å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _format_dialogue_history(self, history: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        if not history:
            return "æ— å¯¹è¯å†å²"
        
        formatted_lines = []
        for entry in history[-10:]:  # åªä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            timestamp = entry.get('timestamp', '')
            role = entry.get('role', '')
            content = entry.get('content', '')
            
            if timestamp and role and content:
                formatted_lines.append(f"[{timestamp}] {role}: {content}")
        
        return '\n'.join(formatted_lines) if formatted_lines else "æ— å¯¹è¯å†å²"


def handle_knowledge_point_dialogue_request(request_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¤„ç†çŸ¥è¯†ç‚¹å¯¹è¯è¯·æ±‚çš„ä¾¿æ·å‡½æ•°
    
    Args:
        request_data: åŒ…å«çŸ¥è¯†ç‚¹æ•°æ®å’Œç”¨æˆ·æ¶ˆæ¯çš„è¯·æ±‚æ•°æ®
        
    Returns:
        Dict: å¤„ç†ç»“æœ
    """
    try:
        # éªŒè¯è¯·æ±‚æ•°æ®
        required_fields = ['knowledge_point_data', 'user_message']
        for field in required_fields:
            if field not in request_data:
                return {
                    'success': False,
                    'error': f'Missing required field: {field}',
                    'response': None
                }
        
        # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
        handler = KnowledgePointDialogueHandler()
        
        # æå–å‚æ•°
        knowledge_point_data = request_data['knowledge_point_data']
        user_message = request_data['user_message']
        dialogue_history = request_data.get('dialogue_history', [])
        dialogue_state = request_data.get('dialogue_state')
        
        # ç”Ÿæˆå›å¤
        language = request_data.get('language', 'English')
        response = handler.generate_response(
            knowledge_point_data=knowledge_point_data,
            user_message=user_message,
            dialogue_history=dialogue_history,
            dialogue_state=dialogue_state,
            language=language
        )
        
        return {
            'success': True,
            'response': response,
            'error': None
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e),
            'response': None
        }


 